# -*- coding: utf-8 -*-
"""search_engine_comparison.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1doOUhnnoQ5HMfVctriG_3d1dhYbvzM98
"""

from bs4 import BeautifulSoup
import time
import requests
from random import randint
from html.parser import HTMLParser
import json
from urllib.parse import urlparse
from scipy.stats import spearmanr
import csv

USER_AGENT = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36'}

from google.colab import files

uploaded = files.upload()

file_path = "100QueriesSet3.txt"
queries = []
with open(file_path, "r") as file:
    for line in file:
        queries.append(line.strip())

class SearchEngine:
    @staticmethod
    def search(query, sleep=True):
        if sleep: # Prevents loading too many pages too soon
            time.sleep(randint(10, 100))
        temp_url = '+'.join(query.split()) # for adding + between words for the query
        url = 'https://www.duckduckgo.com/html/?q=' + temp_url
        soup = BeautifulSoup(requests.get(url, headers=USER_AGENT).text, "html.parser")
        new_results = SearchEngine.scrape_search_result(soup)
        return new_results

    @staticmethod
    def scrape_search_result(soup):
        raw_results = soup.find_all("a", attrs = {"class" : "result__a"})
        results = []
        #implement a check to get only 10 results and also check that URLs must not be duplicated
        for result in raw_results:
            link = result.get('href')
            results.append(link)
        return results

jsonResults = dict()
for query in queries:
    searchResults = SearchEngine.search(query, False)

    if(len(searchResults) >= 10):
        searchResults = searchResults[:10]

    jsonResults[query] = searchResults

# Convert to JSON format
jsonData = json.dumps(jsonResults, indent=2)

# Save to a file
with open("hw1_malhar.json", "w") as jsonFile:
    jsonFile.write(jsonData)

file_path = "Google_Result3.json"
with open(file_path, "r") as file:
    googleJsonData = json.load(file)

file_path = "hw1_malhar.json"
with open(file_path, "r") as file:
    jsonResults = json.load(file)

def similar_urls(url1, url2):
    parsed_url1 = urlparse(url1)
    parsed_url2 = urlparse(url2)

    condition_a = parsed_url1.netloc == parsed_url2.netloc or (
        parsed_url1.netloc.startswith("www.") and parsed_url2.netloc == parsed_url1.netloc[4:]
    ) or (
        parsed_url2.netloc.startswith("www.") and parsed_url1.netloc == parsed_url2.netloc[4:]
    )

    condition_b = parsed_url1.scheme == parsed_url2.scheme

    condition_c = (
        parsed_url1.netloc == parsed_url2.netloc and
        parsed_url1.path.rstrip("/") == parsed_url2.path.rstrip("/")
    )

    are_similar = condition_a and condition_b and condition_c
    # print(parsed_url1, parsed_url2, are_similar)

    return are_similar

def compute_overlap_score(google_results, your_results):
    overlapping_urls = 0
    rank_differences = 0
    for i in range(len(google_results)):
        for j in range(len(your_results)):
            if(similar_urls(your_results[j], google_results[i])):
                rank_differences += ((i - j) ** 2)
                overlapping_urls += 1

    # overlapping_urls = [url for url in your_set if any(similar_urls(url, google_url) for google_url in google_set)]

    # Calculate the percentage overlap
    total_google_results = len(google_results)
    overlap_score = (overlapping_urls / total_google_results) * 100.0

    return overlapping_urls, overlap_score, rank_differences

with open("hw1_malhar.csv", "w", newline="") as csvfile:
    csv_writer = csv.writer(csvfile)

    # Write CSV header
    csv_writer.writerow(["Queries", "Number of Overlapping Results", "Percent Overlap", "Spearman Coefficient"])

    query_num = 1
    sum_n = 0
    sum_percentOverlap = 0
    sum_rho = 0

    for query in jsonResults:
        n, percentOverlap, rank_differences = compute_overlap_score(googleJsonData[query], jsonResults[query])

        if(n == 1):
            if(abs(rank_differences) == 1):
                rho = 1
            elif(abs(rank_differences) == 0):
                rho = 0

        elif(n == 0):
            rho = 0

        else:
            rho = 1 - ((6 * rank_differences) / (n * ((n ** 2) - 1)))

        csv_writer.writerow(["Query " + str(query_num), n, percentOverlap, rho])
        query_num += 1

        sum_n += n
        sum_percentOverlap += percentOverlap
        sum_rho += rho

    avg_n = sum_n / 100
    avg_percentOverlap = sum_percentOverlap / 100
    avg_rho = sum_rho / 100

    csv_writer.writerow(["Averages", avg_n, avg_percentOverlap, avg_rho])

